{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfd730b-4b72-4abe-a000-9eb39364b085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b50018b3-e0e9-48b8-9d4b-6bb6b2bcc91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "env_file = find_dotenv()\n",
    "load_dotenv(env_file, override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970d7428-2ad8-4ac7-a180-63ae4922892d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file(file_path):\n",
    "    import os\n",
    "    base_name, file_extension = os.path.splitext(file_path)\n",
    "\n",
    "    if file_extension == '.pdf':\n",
    "        from langchain.document_loaders import PyPDFLoader\n",
    "        loader = PyPDFLoader(file_path)\n",
    "    elif file_extension == '.docx':\n",
    "        from langchain.document_loaders import Docx2txtLoader\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "    elif file_extension == '.txt':\n",
    "        from langchain.document_loaders import TextLoader\n",
    "        loader = TextLoader(file_path)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    document_data = loader.load()\n",
    "    return document_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e476d8-34be-4bdd-a100-f92b4ab6f8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_from_wikipedia(search_query, language='en', max_docs_to_load=2):\n",
    "    from langchain.document_loaders import WikipediaLoader\n",
    "    wiki_loader = WikipediaLoader(query=search_query, lang=language, load_max_docs=max_docs_to_load)\n",
    "    loaded_data = wiki_loader.load()\n",
    "    return loaded_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d6ca16c-d9c6-415c-aec8-088e025ce000",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(document_data, chunk_length=256):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_length, chunk_overlap=0)\n",
    "    document_chunks = splitter.split_documents(document_data)\n",
    "    return document_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8195472-4950-4239-b7ad-23d172ef8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_embedding_cost(pages):\n",
    "    import tiktoken\n",
    "    tokenizer = tiktoken.encoding_for_model('text-embedding-3-small')\n",
    "    token_count = sum([len(tokenizer.encode(page.page_content)) for page in pages])\n",
    "    print(f'Total Tokens: {token_count}')\n",
    "    print(f'Embedding Cost in USD: {token_count / 1000 * 0.00002:.6f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42a9907f-5c66-43ba-87d8-e4ee6a7be77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_embeddings(index_key, chunk_data):\n",
    "    import pinecone\n",
    "    from langchain_community.vectorstores import Pinecone\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from pinecone import ServerlessSpec\n",
    "    pinecone_api = pinecone.Pinecone(api_key=os.getenv(\"PINECONE_API_KEY\"))\n",
    "    embedding_service = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536, api_key=os.getenv(\"OPEN_AI_KEY\"))\n",
    "    \n",
    "    if index_key in pinecone_api.list_indexes().names():\n",
    "        print(f'Found index {index_key}. Retrieving embeddings ... ', end='')\n",
    "        vector_store_instance = Pinecone.from_existing_index(index_key, embedding_service)\n",
    "        print('Done')\n",
    "    else:\n",
    "        print(f'Index {index_key} not found. Creating and storing embeddings ...', end='')\n",
    "        pinecone_api.create_index(\n",
    "            name=index_key,\n",
    "            dimension=1536,\n",
    "            metric='cosine',\n",
    "            spec=ServerlessSpec(\n",
    "                cloud=\"aws\",\n",
    "                region=\"us-east-1\"\n",
    "            )\n",
    "        )\n",
    "        vector_store_instance = Pinecone.from_documents(chunk_data, embedding_service, index_name=index_key)\n",
    "        print('Done')\n",
    "        \n",
    "    return vector_store_instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00b42761-3fb5-403a-ba9d-8d282b92911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_pinecone_indices(target_index='all'):\n",
    "    import pinecone\n",
    "    pinecone_service = pinecone.Pinecone()\n",
    "    \n",
    "    if target_index == 'all':\n",
    "        indexes_to_delete = pinecone_service.list_indexes().names()\n",
    "        print('Deleting all indices ... ')\n",
    "        for index_name in indexes_to_delete:\n",
    "            pinecone_service.delete_index(index_name)\n",
    "        print('Completed')\n",
    "    else:\n",
    "        print(f'Deleting index {target_index} ...', end='')\n",
    "        pinecone_service.delete_index(target_index)\n",
    "        print('Completed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98fd0cde-64d3-48e9-98a9-77af853aa453",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_and_process_query(vector_store, search_query, top_k_results=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    chat_model = ChatOpenAI(model='gpt-3.5-turbo', temperature=1, api_key=os.getenv(\"OPEN_AI_KEY\"))\n",
    "    search_retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': top_k_results})\n",
    "    qa_pipeline = RetrievalQA.from_chain_type(llm=chat_model, chain_type=\"stuff\", retriever=search_retriever)\n",
    "    result = qa_pipeline.invoke(search_query)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e17f652c-2523-4826-a78e-8c167ac3458f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def divide_text(text_data):\n",
    "    segment_length = 1000\n",
    "    segments = [text_data[i:i + segment_length] for i in range(0, len(text_data), segment_length)]\n",
    "    return segments\n",
    "\n",
    "text = \"This is some example text that we want to split into smaller chunks. \" * 50\n",
    "segments = divide_text(text)\n",
    "print(len(segments))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "884626bb-9cb1-443a-ae98-b981cbc5693c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 22) (3382900417.py, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[11], line 22\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(f\"Estimated embedding cost:\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 22)\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def count_tokens(input_text, model=\"gpt-3.5\"):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    token_count = len(tokenizer.encode(input_text))\n",
    "    return token_count\n",
    "\n",
    "def calculate_cost_per_embedding(text_chunks, rate_per_1000_tokens=0.0004):\n",
    "    total_token_count = 0\n",
    "    for text in text_chunks:\n",
    "        total_token_count += count_tokens(text)\n",
    "    embedding_cost = (total_token_count / 1000) * rate_per_1000_tokens\n",
    "    return embedding_cost\n",
    "\n",
    "text_chunks = [\n",
    "    \"This is the first chunk of text. It's just an example to demonstrate how token counting works.\",\n",
    "    \"Here is the second chunk of text, another example with more content.\",\n",
    "    \"Finally, this is the third chunk of text to make sure we have multiple chunks for the demonstration.\"\n",
    "]\n",
    "\n",
    "estimated_cost = calculate_cost_per_embedding(text_chunks)\n",
    "print(f\"Estimated embedding cost:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bbd8db-18e6-4484-8504-346fa0119fb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
