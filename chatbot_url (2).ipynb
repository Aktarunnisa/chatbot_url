{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "573b08c2-e719-4aa5-831e-acdfdaa34e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea807c19-8c77-4c72-a810-edf53872ae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.washington.edu/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8fd0f53-a470-4ba9-911a-eddd36838a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def fetch_content(url, headers):\n",
    "    response = requests.get(url, headers=headers)\n",
    "    print(f\"Fetching {url} - Status Code: {response.status_code}\")\n",
    "    return response\n",
    "\n",
    "def is_successful(response):\n",
    "    return response.status_code == 200\n",
    "\n",
    "def extract_headings(soup):\n",
    "    headings = []\n",
    "    for heading in range(1, 7):\n",
    "        for tag in soup.find_all(f\"h{heading}\"):\n",
    "            text = tag.get_text(strip=True)\n",
    "            if text:\n",
    "                headings.append(text)\n",
    "    return list(dict.fromkeys(headings))\n",
    "\n",
    "def extract_paragraphs(soup):\n",
    "    paragraphs = []\n",
    "    for p in soup.find_all(\"p\"):\n",
    "        text = p.get_text(strip=True)\n",
    "        if text:\n",
    "            paragraphs.append(text)\n",
    "    return list(dict.fromkeys(paragraphs))\n",
    "\n",
    "def extract_other_text(soup):\n",
    "    other_text = []\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.name not in [\"h1\", \"h2\", \"h3\", \"h4\", \"h5\", \"h6\", \"p\"]:\n",
    "            text = tag.get_text(strip=True)\n",
    "            if text:\n",
    "                other_text.append(text)\n",
    "    return list(dict.fromkeys(other_text))\n",
    "\n",
    "def scrape_data(url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36\"\n",
    "    }\n",
    "    response = fetch_content(url, headers)\n",
    "    if not is_successful(response):\n",
    "        return {\"url\": url, \"error\": f\"Failed to fetch content, status code: {response.status_code}\"}\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    return {\n",
    "        \"url\": url,\n",
    "        \"headings\": extract_headings(soup),\n",
    "        \"paragraphs\": extract_paragraphs(soup),\n",
    "        \"other_text\": extract_other_text(soup)\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b06c112-2e8f-4a1d-b7db-96bb7a1385af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, output_dir):\n",
    "    domain = data[\"url\"].split(\"//\")[-1].split(\"/\")[0]\n",
    "    file_name = f\"{domain}.json\"\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Data saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a91312e-8159-47e2-8379-f7718574117c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Lenovo'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53740dd0-f7aa-4117-9036-c360b4f76be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows-SSD\n",
      " Volume Serial Number is 76D0-2D19\n",
      "\n",
      " Directory of C:\\Users\\Lenovo\n",
      "\n",
      "12/20/2024  12:29 PM    <DIR>          .\n",
      "12/11/2024  06:23 PM    <DIR>          ..\n",
      "04/29/2024  11:08 AM    <DIR>          .cache\n",
      "05/22/2024  10:27 AM    <DIR>          .eclipse\n",
      "09/25/2022  11:16 PM    <DIR>          .idlerc\n",
      "12/19/2024  07:48 PM    <DIR>          .ipynb_checkpoints\n",
      "12/19/2024  04:40 PM    <DIR>          .ipython\n",
      "12/19/2024  05:37 PM    <DIR>          .jupyter\n",
      "09/27/2022  12:11 PM    <DIR>          .ms-ad\n",
      "12/14/2024  11:34 AM    <DIR>          .p2\n",
      "12/25/2022  09:53 PM               178 .packettracer\n",
      "04/29/2024  11:07 AM    <DIR>          .redhat\n",
      "04/18/2024  11:23 AM    <DIR>          .spss\n",
      "07/26/2024  01:04 PM    <DIR>          .VirtualBox\n",
      "04/29/2024  10:56 AM    <DIR>          .vscode\n",
      "11/24/2022  07:56 PM    <DIR>          Cisco Packet Tracer 7.3.0\n",
      "12/11/2024  06:31 PM    <DIR>          Contacts\n",
      "12/11/2024  06:28 PM    <DIR>          Desktop\n",
      "08/13/2023  05:04 PM    <DIR>          Documents\n",
      "12/20/2024  12:24 PM    <DIR>          Downloads\n",
      "10/18/2024  07:24 AM    <DIR>          eclipse\n",
      "12/11/2024  06:31 PM    <DIR>          Favorites\n",
      "12/11/2024  06:31 PM    <DIR>          Links\n",
      "12/11/2024  06:31 PM    <DIR>          Music\n",
      "12/11/2024  06:28 PM    <DIR>          OneDrive\n",
      "10/19/2024  03:08 PM    <DIR>          Pictures\n",
      "12/11/2024  06:31 PM    <DIR>          Saved Games\n",
      "12/19/2024  06:43 PM    <DIR>          scraped_data\n",
      "12/11/2024  06:31 PM    <DIR>          Searches\n",
      "12/19/2024  07:48 PM    <DIR>          Untitled Folder\n",
      "12/19/2024  05:36 PM           188,333 Untitled.ipynb\n",
      "12/20/2024  12:29 PM             3,426 Untitled1.ipynb\n",
      "12/19/2024  07:50 PM               617 Untitled2.ipynb\n",
      "12/19/2024  07:13 PM            54,527 url_chatbot.ipynb\n",
      "12/11/2024  06:31 PM    <DIR>          Videos\n",
      "               5 File(s)        247,081 bytes\n",
      "              30 Dir(s)  60,162,068,480 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781a84a5-6726-4fa1-a49d-076313c254cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scrapeData' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m output_directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./scraped_data\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m scraped_data \u001b[38;5;241m=\u001b[39m \u001b[43mscrapeData\u001b[49m(url)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(json\u001b[38;5;241m.\u001b[39mdumps(scraped_data, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m scraped_data:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scrapeData' is not defined"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def scrapeData(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        # Extract desired data from the website\n",
    "        title = soup.title.string if soup.title else \"No title found\"\n",
    "        body_text = soup.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"title\": title,\n",
    "            \"content\": body_text\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {\n",
    "            \"url\": url,\n",
    "            \"error\": f\"Error while accessing the URL: {e}\"\n",
    "        }\n",
    "\n",
    "output_directory = \"./scraped_data\"\n",
    "url = \"https://example.com\"\n",
    "\n",
    "scraped_data = scrapeData(url)\n",
    "print(json.dumps(scraped_data, indent=4))\n",
    "\n",
    "if \"error\" not in scraped_data:\n",
    "    save_to_json(scraped_data, output_directory)\n",
    "else:\n",
    "    print(scraped_data[\"error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19987a11-267b-42a8-ad5c-d894d9ec3dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
